{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ac-xCS6WLlCM"
      },
      "source": [
        "# 📌 Machine Learning Assignment 1 - Instructions & Guidelines\n",
        "\n",
        "### **📝 General Guidelines**\n",
        "Welcome to Machine Learning Assignment 1! This assignment will test your understanding of **regression and classification models**, including **data preprocessing, hyperparameter tuning, and model evaluation**.\n",
        "\n",
        "Follow the instructions carefully, and ensure your implementation is **correct, well-structured, and efficient**.\n",
        "\n",
        "🔹 **Submission Format:**  \n",
        "- Your submission **must be a single Jupyter Notebook (.ipynb)** file.  \n",
        "- **File Naming Convention:**  \n",
        "  - Use **your university email as the filename**, e.g.,  \n",
        "    ```\n",
        "    j.doe@innopolis.university.ipynb\n",
        "    ```\n",
        "  - **Do NOT modify this format**, or your submission may not be graded.\n",
        "\n",
        "🔹 **Assignment Breakdown:**\n",
        "| Task | Description | Points |\n",
        "|------|------------|--------|\n",
        "| **Task 1.1** | Linear Regression | 20 |\n",
        "| **Task 1.2** | Polynomial Regression | 20 |\n",
        "| **Task 2.1** | Data Preprocessing | 15 |\n",
        "| **Task 2.2** | Model Comparison | 45 |\n",
        "| **Total** | - | **100** |\n",
        "\n",
        "---\n",
        "\n",
        "### **📂 Dataset & Assumptions**\n",
        "The dataset files are stored in the `datasets/` folder.  \n",
        "- **Regression Dataset:** `datasets/task1_data.csv`\n",
        "- **Classification Dataset:** `datasets/pokemon_modified.csv`\n",
        "\n",
        "Each dataset is structured as follows:\n",
        "\n",
        "🔹 **`task1_data.csv` (for regression tasks)**  \n",
        "- Contains `X_train`, `y_train`, `X_test`, and `y_test`.  \n",
        "- The goal is to fit **linear and polynomial regression models** and evaluate their performance.  \n",
        "\n",
        "🔹 **`pokemon_modified.csv` (for classification tasks)**  \n",
        "- Contains Pokémon attributes, with `is_legendary` as the **binary target variable (0 or 1)**.  \n",
        "- Some features contain **missing values** and **categorical variables**, requiring preprocessing.\n",
        "\n",
        "---\n",
        "\n",
        "### **🚀 How to Approach the Assignment**\n",
        "1. **Start with Regression (Task 1)**\n",
        "   - Implement **linear regression** and **polynomial regression**.\n",
        "   - Use **GridSearchCV** for polynomial regression to find the best degree.\n",
        "   - Evaluate using **MSE, RMSE, MAE, and R² Score**.\n",
        "\n",
        "2. **Move to Data Preprocessing (Task 2.1)**\n",
        "   - Load and clean the Pokémon dataset.\n",
        "   - Handle **missing values** correctly.\n",
        "   - Encode categorical variables properly.\n",
        "   - Ensure **no data leakage** when doing the preprocessing.\n",
        "\n",
        "3. **Train and Evaluate Classification Models (Task 2.2)**\n",
        "   - Train **Logistic Regression, KNN, and Naive Bayes**.\n",
        "   - Use **GridSearchCV** for hyperparameter tuning.\n",
        "   - Evaluate models using **Accuracy, Precision, Recall, and F1-score**.\n",
        "\n",
        "---\n",
        "\n",
        "### **📌 Grading & Evaluation**\n",
        "- Your notebook will be **autograded**, so ensure:\n",
        "  - Your function names **exactly match** the given specifications.\n",
        "  - Your output format matches the expected results.\n",
        "- Partial credit will be given where applicable.\n",
        "\n",
        "🔹 **Need Help?**  \n",
        "- If you have any questions, refer to the **assignment markdown instructions** in each task before asking for clarifications.\n",
        "- You can post your question on this [Google sheet](https://docs.google.com/spreadsheets/d/1oyrqXDjT2CeGYx12aZhZ-oDKcQQ-PCgT91wHPhTlBCY/edit?usp=sharing)\n",
        "\n",
        "🚀 **Good luck! Happy coding!** 🎯"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dS55bb6DLlCO"
      },
      "source": [
        "### FAQ\n",
        "\n",
        "**1) Should we include the lines to import the libraries?**\n",
        "\n",
        "- **Answer:**  \n",
        "  It doesn't matter if you include extra import lines, as the grader will only call the specified functions.\n",
        "\n",
        "**2) Is it okay to submit my file with code outside of the functions?**\n",
        "\n",
        "- **Answer:**  \n",
        "  Yes, you can include additional code outside of the functions as long as the entire script runs correctly when converted to a `.py` file.\n",
        "\n",
        "**Important Clarification:**\n",
        "\n",
        "- The grader will first convert the Jupyter Notebook (.ipynb) into a Python file (.py) and then run it.\n",
        "- **Note:** Please do not include any commands like `!pip install numpy` because they may break the conversion process and therefore the submission will not be graded."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "08sHFFqCLlCP"
      },
      "source": [
        "## Task 1: Linear and Polynomial Regression (30 Points)\n",
        "\n",
        "### Task 1.1 - Linear Regression (15 Points)\n",
        "#### **Instructions**\n",
        "1. Load the dataset from **`datasets/task1_data.csv`**.\n",
        "2. Extract training and testing data from the following columns:\n",
        "   - `\"X_train\"`: Training feature values.\n",
        "   - `\"y_train\"`: Training target values.\n",
        "   - `\"X_test\"`: Testing feature values.\n",
        "   - `\"y_test\"`: Testing target values.\n",
        "3. Train a **linear regression model** on `X_train` and `y_train`.\n",
        "4. Use the trained model to predict `y_test` values.\n",
        "5. Compute and return the following **evaluation metrics** as a dictionary:\n",
        "   - **Mean Squared Error (MSE)**\n",
        "   - **Root Mean Squared Error (RMSE)**\n",
        "   - **Mean Absolute Error (MAE)**\n",
        "   - **R² Score**\n",
        "6. The function signature should match:\n",
        "   ```python\n",
        "   def task1_linear_regression() -> Dict[str, float]:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6u7AAKcaLlCP"
      },
      "source": [
        "Please do not use any other libraries except for the ones imported below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "68tssINhLlCQ"
      },
      "outputs": [],
      "source": [
        "# Standard Library Imports\n",
        "import os\n",
        "import importlib.util\n",
        "import nbformat\n",
        "from tempfile import NamedTemporaryFile\n",
        "from typing import Tuple, Dict\n",
        "\n",
        "\n",
        "# Third-Party Library Imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "from nbconvert import PythonExporter\n",
        "\n",
        "# Scikit-Learn Imports\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler, PolynomialFeatures, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import (accuracy_score, precision_score, recall_score, f1_score,\n",
        "                             mean_squared_error, mean_absolute_error, r2_score)\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import GaussianNB"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "InbmzOg-LlCR"
      },
      "outputs": [],
      "source": [
        "def task1_linear_regression() -> Dict[str, float]:\n",
        "\n",
        "  # Load the dataset from datasets/task1_data.csv\n",
        "\n",
        "  df = pd.read_csv(\"datasets/task1_data.csv\")\n",
        "\n",
        "  # Extract training and testing data\n",
        "  X_train = df.iloc[:, 0].values.reshape(-1, 1)\n",
        "  y_train = df.iloc[:, 1].values.reshape(-1, 1)\n",
        "  X_test = df.iloc[:, 2].values.reshape(-1, 1)\n",
        "  y_test = df.iloc[:, 3].values.reshape(-1, 1)\n",
        "\n",
        "  # Train a linear regression model on `X_train, y_train`\n",
        "  regressor = LinearRegression()\n",
        "  regressor.fit(X_train, y_train)\n",
        "\n",
        "  # Use the trained model to predict `y_test` values\n",
        "  y_pred = regressor.predict(X_test)\n",
        "\n",
        "  # Compute evaluation metrics: **MSE, RMSE, MAE, R² Score**\n",
        "\n",
        "  mse = mean_squared_error(y_test, y_pred)\n",
        "  rmse = np.sqrt(mse)\n",
        "  mae = mean_absolute_error(y_pred, y_test)\n",
        "  r2 = r2_score(y_test,y_pred)\n",
        "\n",
        "\n",
        "\n",
        "  return {\n",
        "        \"MSE\": mse,\n",
        "        \"RMSE\": rmse,\n",
        "        \"MAE\": mae,\n",
        "        \"R2\": r2\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LU4cJ7SdXnRb",
        "outputId": "b99ea368-95ca-4c37-b8b0-aea7532c8769"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'MSE': 0.78105677092199,\n",
              " 'RMSE': 0.8837741628504365,\n",
              " 'MAE': 0.7837610302414408,\n",
              " 'R2': 0.2609450135378707}"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "linear_model = task1_linear_regression()\n",
        "linear_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUQAQjR-LlCR"
      },
      "source": [
        "### Task 1.2 - Polynomial Regression (15 Points)\n",
        "\n",
        "#### **Instructions**\n",
        "1. Load the dataset from **`datasets/task1_data.csv`**.\n",
        "2. Extract training and testing data from the following columns:\n",
        "   - `\"X_train\"`: Training feature values.\n",
        "   - `\"y_train\"`: Training target values.\n",
        "   - `\"X_test\"`: Testing feature values.\n",
        "   - `\"y_test\"`: Testing target values.\n",
        "3. Define a **pipeline** that includes:\n",
        "   - **Polynomial feature transformation** (degree range: **2 to 10**).\n",
        "   - **Linear regression model**.\n",
        "4. Use **GridSearchCV** with **8-fold cross-validation** to determine the best polynomial degree.\n",
        "5. Train the model with the best polynomial degree and **evaluate it on the test set**.\n",
        "6. Compute and return the following results as a dictionary:\n",
        "   - **Best polynomial degree** (`best_degree`)\n",
        "   - **Mean Squared Error (MSE)**\n",
        "\n",
        "#### **Function Signature**\n",
        "```python\n",
        "def task1_polynomial_regression() -> Dict[str, float]:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "5rL-XtDjLlCR"
      },
      "outputs": [],
      "source": [
        "def task1_polynomial_regression() -> Dict[str, float]:\n",
        "\n",
        "  # Load the dataset from datasets/task1_data.csv\n",
        "  df = pd.read_csv(\"datasets/task1_data.csv\")\n",
        "\n",
        "  # Extract training and testing data\n",
        "  X_train = df.iloc[:, 0].values.reshape(-1, 1)\n",
        "  y_train = df.iloc[:, 1].values.reshape(-1, 1)\n",
        "  X_test = df.iloc[:, 2].values.reshape(-1, 1)\n",
        "  y_test = df.iloc[:, 3].values.reshape(-1, 1)\n",
        "\n",
        "  # Define a **pipeline** with polynomial feature transformation and linear regression\n",
        "  degrees = np.arange(2,11)\n",
        "\n",
        "  # Define pipline\n",
        "  polynomial_features = PolynomialFeatures()\n",
        "  linear_regression = LinearRegression()\n",
        "\n",
        "  pipline = Pipeline([\n",
        "         (\"polynomial_features\", polynomial_features),\n",
        "         (\"linear_regression\", linear_regression)\n",
        "    ])\n",
        "\n",
        "  # Set up the parameter grid for GridSearchCV to search over polynomial degrees\n",
        "  param_grid = {'polynomial_features__degree': degrees}\n",
        "\n",
        "  # Train the best polynomial regression model and evaluate its performance.\n",
        "  grid_search = GridSearchCV(pipline, param_grid, cv = 8, scoring='neg_mean_squared_error')\n",
        "  grid_search.fit(X_train, y_train)\n",
        "\n",
        "  best_degree = grid_search.best_params_['polynomial_features__degree']\n",
        "\n",
        "  y_pred = grid_search.best_estimator_.predict(X_test)\n",
        "\n",
        "  mse = mean_squared_error(y_test, y_pred)\n",
        "\n",
        "  return {\n",
        "      \"best_degree\": best_degree,\n",
        "      \"MSE\": mse\n",
        "  }\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kaBEL31liA5a",
        "outputId": "12618445-b7de-413b-ee07-175eba7a2332"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'best_degree': 2, 'MSE': 0.08205877217937993}"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "polynomial_model = task1_polynomial_regression()\n",
        "polynomial_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Or_CLwxtLlCS"
      },
      "source": [
        "## Task 2: Classification with Data Preprocessing (70 Points)\n",
        "\n",
        "### Task 2.1 - Data Preprocessing (30 Points)\n",
        "\n",
        "#### **Instructions**\n",
        "1. Load the dataset from **`datasets/pokemon_modified.csv`**.\n",
        "2. Look at the data and study the provided features\n",
        "3. Remove the **two redundant features**\n",
        "4. Handle **missing values**:\n",
        "   - Use **mean imputation** for **\"height_m\"** and **\"weight_kg\"**.\n",
        "   - Use **median imputation** for **\"percentage_male\"**.\n",
        "5. Perform **one-hot encoding** for the categorical column **\"type1\"**.\n",
        "6. Ensure the **target variable** (`\"is_legendary\"`) is present.\n",
        "7. **Split the data into training and testing sets** (`80%-20%` split). Is it balanced?\n",
        "8. **Apply feature scaling** using **StandardScaler** or **MinMaxScaler**.\n",
        "9. Return the following:\n",
        "   - `X_train_scaled`: Processed training features.\n",
        "   - `X_test_scaled`: Processed testing features.\n",
        "   - `y_train`: Training labels.\n",
        "   - `y_test`: Testing labels.\n",
        "\n",
        "#### **Function Signature**\n",
        "```python\n",
        "def task2_preprocessing() -> Tuple[pd.DataFrame, pd.DataFrame, pd.Series, pd.Series]:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "cKKLyI1yLlCS"
      },
      "outputs": [],
      "source": [
        "def task2_preprocessing() -> Tuple[pd.DataFrame, pd.DataFrame, pd.Series, pd.Series]:\n",
        "\n",
        "  # Load the dataset\n",
        "  data = pd.read_csv('datasets/pokemon_modified.csv')\n",
        "\n",
        "  # Remove redundant columns(Name and generation of poke don't bring any useful info for determinig legendary)\n",
        "  data.drop(columns=[\"name\", \"classification\"], inplace=True)\n",
        "\n",
        "  # Handle missing values\n",
        "  imputer_mean = SimpleImputer(strategy='mean')\n",
        "  imputer_median = SimpleImputer(strategy='median')\n",
        "\n",
        "  data['height_m'] = imputer_mean.fit_transform(data[['height_m']])\n",
        "  data['weight_kg'] = imputer_mean.fit_transform(data[['weight_kg']])\n",
        "  data['percentage_male'] = imputer_median.fit_transform(data[['percentage_male']])\n",
        "\n",
        "  # Perform **one-hot encoding** on `\"type1\"`\n",
        "  encoder = OneHotEncoder(sparse_output=False, drop='first')\n",
        "  type1_encoded = encoder.fit_transform(data[['type1']])\n",
        "  type1_df = pd.DataFrame(type1_encoded, columns=encoder.get_feature_names_out(['type1']))\n",
        "\n",
        "  # Merge the one-hot encoded columns back to the original dataframe\n",
        "  data = pd.concat([data, type1_df], axis=1)\n",
        "  data.drop(columns=['type1'], inplace=True)\n",
        "\n",
        "  # Split dataset into features X and target y\n",
        "  X = data.drop(columns=['is_legendary'])\n",
        "  y = data['is_legendary']\n",
        "\n",
        "  # Split the dataset into **80% training, 20% testing** using **stratification** to maintain class balance\n",
        "  X_train, x_test, y_train, y_test = train_test_split(X,y, test_size=0.2, stratify=y, random_state=42)\n",
        "\n",
        "  # Apply feature scaling (**StandardScaler**)\n",
        "  scaler = MinMaxScaler()\n",
        "  scaler.fit(X_train)\n",
        "  X_train = pd.DataFrame(scaler.transform(X_train), columns=X_train.columns)\n",
        "  x_test = pd.DataFrame(scaler.transform(x_test), columns=x_test.columns)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  return X_train, x_test, y_train, y_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOhlKrBhLlCT"
      },
      "source": [
        "### Task 2.2 - Model Comparison (40 Points)\n",
        "\n",
        "#### **Instructions**\n",
        "1. **Train three classification models** on the preprocessed dataset:\n",
        "   - **Logistic Regression**\n",
        "   - **K-Nearest Neighbors (KNN)**\n",
        "   - **Gaussian Naive Bayes (GNB)**\n",
        "2. Use **GridSearchCV** for **hyperparameter tuning** on:\n",
        "   - **Logistic Regression**: Regularization strength (`C`) and penalty (`l1`, `l2`).\n",
        "   - **KNN**: Number of neighbors (`n_neighbors`), weight function, and distance metric.\n",
        "3. Train each model on the **training set** and evaluate on the **test set**.\n",
        "4. Compute the following **evaluation metrics**:\n",
        "   - **Accuracy**\n",
        "   - **Precision**\n",
        "   - **Recall**\n",
        "   - **F1 Score**\n",
        "5. Return a dictionary containing the evaluation metrics for each model.\n",
        "\n",
        "#### **Function Signature**\n",
        "```python\n",
        "def task2_model_comparison() -> Dict[str, Dict[str, float]]:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "wrhsZ1CHLlCT"
      },
      "outputs": [],
      "source": [
        "def task2_model_comparison() -> Dict[str, Dict[str, float]]:\n",
        "\n",
        "  # Load the preprocessed dataset from `task2_preprocessing()`\n",
        "  X_train, X_test, y_train, y_test = task2_preprocessing()\n",
        "\n",
        "  # Logistic Regression\n",
        "  logistic_regression = LogisticRegression()\n",
        "  knn = KNeighborsClassifier()\n",
        "  dnb = GaussianNB()\n",
        "\n",
        "  logistic_regression_params = [\n",
        "\n",
        "      {\n",
        "        'penalty': ['l1'],\n",
        "        'solver': ['liblinear', 'saga'],\n",
        "        'C': [0.001, 0.01, 0.1, 1, 10, 100]\n",
        "    },\n",
        "    {\n",
        "        'penalty': ['l2'],\n",
        "        'solver': ['lbfgs', 'liblinear', 'newton-cg', 'newton-cholesky', 'sag', 'saga'],\n",
        "        'C': [0.001, 0.01, 0.1, 1, 10, 100]\n",
        "    },\n",
        "    {\n",
        "        'penalty': [None],\n",
        "        'solver': ['newton-cholesky', 'lbfgs', 'sag', 'saga'],\n",
        "        'C': [0.001, 0.01, 0.1, 1, 10, 100]\n",
        "    }\n",
        "  ]\n",
        "\n",
        "\n",
        "  knn_param_grid = {\n",
        "    'n_neighbors': list(range(1, 11)),\n",
        "    'weights': ['uniform', 'distance'],\n",
        "    'metric': ['euclidean', 'manhattan', 'chebyshev']\n",
        "    }\n",
        "\n",
        "    #  GridSearchCV\n",
        "  logistic_regression_grid_search = GridSearchCV(logistic_regression, logistic_regression_params, cv=8, n_jobs=-1)\n",
        "  knn_grid_search = GridSearchCV(knn, knn_param_grid,cv=8, n_jobs=-1)\n",
        "\n",
        "    # Train models\n",
        "  logistic_regression_grid_search.fit(X_train, y_train)\n",
        "  knn_grid_search.fit(X_train, y_train)\n",
        "  dnb.fit(X_train, y_train)\n",
        "\n",
        "  models = {\n",
        "        \"Logistic Regression\": logistic_regression_grid_search.best_estimator_,\n",
        "        \"KNN\": knn_grid_search.best_estimator_,\n",
        "        \"Naive Bayes\": dnb\n",
        "    }\n",
        "\n",
        "  evaluation_metrics = {}\n",
        "\n",
        "  for model_name, model in models.items():\n",
        "      y_pred = model.predict(X_test)\n",
        "\n",
        "        # Calculate metrics\n",
        "      metrics = {\n",
        "          \"accuracy\": accuracy_score(y_test, y_pred),\n",
        "          \"precision\": precision_score(y_test, y_pred),\n",
        "          \"recall\": recall_score(y_test, y_pred),\n",
        "          \"f1_score\": f1_score(y_test, y_pred)\n",
        "        }\n",
        "      evaluation_metrics[model_name] = metrics\n",
        "\n",
        "  return evaluation_metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kJc-QzBfyMes",
        "outputId": "02a89fab-36ef-4bbd-8e51-cec051a4ffaa"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'Logistic Regression': {'accuracy': 0.9875776397515528,\n",
              "  'precision': 1.0,\n",
              "  'recall': 0.8571428571428571,\n",
              "  'f1_score': 0.9230769230769231},\n",
              " 'KNN': {'accuracy': 0.9751552795031055,\n",
              "  'precision': 0.9166666666666666,\n",
              "  'recall': 0.7857142857142857,\n",
              "  'f1_score': 0.8461538461538461},\n",
              " 'Naive Bayes': {'accuracy': 0.906832298136646,\n",
              "  'precision': 0.48148148148148145,\n",
              "  'recall': 0.9285714285714286,\n",
              "  'f1_score': 0.6341463414634146}}"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "classification_model = task2_model_comparison()\n",
        "classification_model"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}